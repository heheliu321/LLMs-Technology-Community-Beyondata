## 《大模型Agent开发实战》（体验课）

# 多模态RAG引擎开发实战

# Part 1.多模态RAG技术体系介绍

- 本期公开课四大模块内容

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828194616411.png" alt="image-20250828194616411" style="zoom:50%;" />

- 【演示】实操项目一：从零到一快速搭建多模态RAG系统

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/74cfd666d005af475500d97302823538_raw.mp4"></video>

- 【演示】实操项目二：企业级多模态RAG系统开发实战

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/27f4b2e749af80e62b1a9e3900e30e3f_raw.mp4"></video>

- 课件&代码&项目源码领取：

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/f7c49313c41eaeb3a2b3b9e9240d9f1e.png" alt="f7c49313c41eaeb3a2b3b9e9240d9f1e" style="zoom:50%;" />

- 本节目录

[toc]

## 一、RAG（Retrieval-Augmented Generation，检索增强生成）技术回顾

&emsp;&emsp;RAG，Retrieval-Augmented Generation，也被称作检索增强生成技术，最早在 Facebook AI（Meta AI）在 2020 年发表的论文《Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks》（ https://arxiv.org/abs/2005.11401 ）中正式提出，这种方法的核心思想是借助一些文本检索策略，让大模型每次问答前都带入相关文本，以此来改善大模型回答时的准确性。这项技术刚发布时并未引发太大关注，而伴随2022年大模型技术大爆发，RAG技术才逐渐进入人们视野，并且由于早期大模型技术应用均已“知识库问答”为主，而RAG技术是最易上手、并且上限极高的技术，因此很快就成为了大模型技术人必备的技术之一。

### 1. RAG技术实现一般流程

​	对于初学者来说，为了更好的上手学习RAG技术，我们首先需要对RAG技术最简单的实现形式有个基础的了解。一个最简单的RAG技术实现流程如下所示：

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20231218182814731.png" alt="image-20231218182814731" style="zoom:33%;" />


​	我们需要围绕给定的文档（往往是非常长的文档）先进行切分，然后将切分的文档转化为计算机能识别的形式，也就是将其转化为一个数值型向量（也被称为词向量），然后当用户询问问题的时候，我们再将用户的问题转化为词向量，并和段落文档的词向量进行相似度匹配，借此找出和当前用户问题最相关的原始文档片段，然后将用户的问题和匹配的到的原文片段都带入大模型，进行最终的问答。由此便可实现一次完整的文档检索增强执行流程。

&emsp;&emsp;具体执行过程如下所示：

<center><center><img src="https://pictes.oss-cn-beijing.aliyuncs.com/LLM/image-20250708015352331.png" alt="image-20250708015352331" style="zoom:50%;" />

### 3. RAG技术核心应用场景：拓展模型知识边界与减少问答幻觉

&emsp;&emsp;那这样的一个检索增强流程到底有什么用呢？这就不得不从当代大模型本身的三项技术缺陷开始说起了。

- 缺陷一：大模型幻觉

&emsp;&emsp;相信大家在使用大模型的时候，都会遇到大模型无中生有胡编乱造答案的情况，例如胡乱生成一些概念、一些论文甚至是一些实时等，这就是所谓的大模型幻觉。

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202507072338105.png" alt="image-20250707233838007" style="zoom:50%;" />



而其中，第一代DeepSeek R1模型的幻觉是非常严重的，平均七次回答中就会有一次的回答存在幻觉，这可以说是第一版R1模型最大的短板。

&emsp;&emsp;大型语言模型之所以会产生幻觉，主要是因为它们的训练方式和内在机制决定了它们并不具备真正理解和验证事实的能力。模型在训练过程中，通过分析大规模文本数据来学习不同词语和句子之间的概率关系，也就是在某种程度上掌握“在什么上下文中，什么样的回答听起来更合理”。然而，模型并没有接入实时的知识库或事实核查工具，当它遇到陌生的问题、模糊的描述或者上下文不完整的输入时，就会基于概率和语料库中似是而非的关联去“编造”一个看似正确的答案。由于这些输出往往语法流畅、逻辑连贯，人类读者很容易误以为它是真实可信的内容，这就是我们通常说的“模型幻觉”。

- 缺陷二：有限的最大上下文

&emsp;&emsp;而除此之外，大模型在实际应用中还会另一个“障碍”，那就是最大上下文限制。由于大模型的本质其实是一个算法，不管是让大模型“知道”有哪些外部工具，还是要给大模型进行“背景设置”，或者是要给模型添加历史对话消息，以及本次对话的输出，都需要占用这个上下文窗口。这就使得我们在一次对话中能够给大模型灌输的知识（文本）其实是有限的。

&emsp;&emsp;大型语言模型还存在最大上下文限制，这是由它们的架构和计算方式决定的。每次生成回答时，模型需要把输入文本转换成固定长度的数字序列（称为token），并在内部一次性加载到模型的“上下文窗口”中进行处理。这个窗口的大小是有限的，不同模型一般在几千到几万token之间。如果输入内容超出这个长度，模型要么截断最前面的部分，要么丢弃部分信息，这就会造成对话历史、长文档或先前提到的重要细节的遗失。因为它无法跨越上下文窗口无限地保留信息，所以在面对长对话或者大量背景知识时，模型常常出现上下文断裂、回答不连贯或者忽略先前条件的情况。

&emsp;&emsp;早些时候的大模型普遍是8k最大上下文，相当于是8-10页中文PDF，伴随着大模型预训练技术的不断发展，顶尖的大模型，如Gemini 2.5 Pro和GPT-4.1等模型，已经达到了1M的最大上下文长度，相当于是一千页的PDF，相当于1.5本《红楼梦》，而普通的模型，也基本达到64K或128K最大上下文，相当于60-100也左右的PDF。

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202507072348046.png" alt="image-20250707234800964" style="zoom:33%;" />



但是，模型上下文的增长也是有限度的，对于开发者来说，能够一次性输入的信息都会有限制。

- 缺陷三：模型专业知识与时效性知识不足

&emsp;&emsp;大型语言模型虽然在通用领域展现出令人瞩目的语言理解和生成能力，但其在特定领域的专业知识掌握往往存在明显局限。其根本原因在于，模型的训练依赖于预先收集的大规模语料，这些语料覆盖面虽广，却很难保证在所有专业领域中具有足够的深度和准确性。某些领域，如医学、法律或前沿科技，知识更新速度快且门槛较高，公开可获取的高质量数据本身就有限，模型难以在此基础上形成系统性和权威性的认知。此外，模型训练通常在固定的时间点结束，因此其所掌握的知识具有天然的时效性，无法实时反映新近出现的研究成果、政策变化或行业动态。这种静态的知识存储模式，决定了大模型在面对最新或高度专业化的问题时，往往难以提供全面、精确的解答。

<img src="https://pictes.oss-cn-beijing.aliyuncs.com/LLM/image-20250708022912484.png" alt="image-20250708022912484" style="zoom:50%;" />

&emsp;&emsp;基于此，我们再回顾RAG的技术实现流程，就不难发现其背后的技术价值了：如果我们能在每次对话的时候，为当前模型输入最精准的问题相关的文档，那就能拓展模型的知识边界，无论是提升模型专业知识的准确性、给模型灌输一些时效性的知识、或者消除模型幻觉，都将大有助益，而在其他一些对话场景中，无论是需要围绕海量的文本搭建本地问答知识库、还是在构建无限上下文的聊天机器人，RAG技术都是最佳解决方案。

### 4. 当前问答机器人标配：RAG系统

&emsp;&emsp;正因为知识库检索的广泛的使用需求，RAG技术几乎成了现在各项聊天机器人的标配，无论是面向普通用户的聊天问答应用Cherry Studio：

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250708141040865.png" alt="image-20250708141040865" style="zoom:33%;" />

还是面向企业应用场景的通用开源前端Open-WebUI:

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250708142841404.png" alt="image-20250708142841404" style="zoom:33%;" />

都毫无例外都配置了RAG功能，而对于OpenAI-WebUI这种企业级前端，还为用户展示了RAG检索过程诸多技术细节：

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250708143007552.png" alt="image-20250708143007552" style="zoom:33%;" />

尽管这些项目能让用户更加快速的使用RAG系统，但这种传统的RAG流程（也被称作Native RAG），在长期的应用过程中也逐渐展露出很多问题，例如对于非结构化的文本（例如包含图片、公式的文本）无法进行检索，而对于超大规模文本的检索又会存在精度不足、或者无法提炼总结跨文本概念等问题。为此，近两年的时间里，在无数技术人的共同努力下，RAG技术有了长足的成长和突破。

&emsp;&emsp;我们团队自研的开源Jupyter智能体助教MateGen Air，也提供了完整的公开课（部分）知识库问答功能：

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250708155628356.png" alt="image-20250708155628356" style="zoom:33%;" />

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250708155442209.png" alt="image-20250708155442209" style="zoom:33%;" />

### 5. RAG全栈技术体系介绍

&emsp;&emsp;但是，RAG技术其实是一项应用面广、门槛很低、但同时上限也很高的一项技术。历经数年的技术发展，RAG技术的体系已经非常庞大，以下是RAG技术全栈技术框架概览：

<center><img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/88bac0891ac369368fd9199d1542862.png" alt="88bac0891ac369368fd9199d1542862" style="zoom:50%;" />

- 高精度大范围语义检索：GraphRAG

**GraphRAG（Graph-enhanced Retrieval-Augmented Generation）** 是一种在经典 RAG 基础上引入**知识图谱/图结构**的新型检索生成方法 。其核心思想是通过将文档或数据转换成图的形式，从而捕捉实体与实体之间的语义关系，并在检索阶段利用图遍历、关系推理等机制来辅助上下文构建，这种结构化信息能够提升语义理解和多跳推理能力。

具体来说，GraphRAG 的流程包括：

1. **图谱构建**：将文本拆分为多个单元（TextUnit），提取实体与关系，构造知识图，并进行图社区检测与摘要；
2. **混合检索**：用户提问既可以进行向量检索定位实体，也可以通过图查询（如 Cypher/SPARQL）沿关系边扩展信息 ；
3. **图增强生成**：将检索到的节点、路径、社区摘要等信息拼接进 Prompt，引导 LLM 生成更准确、结构清晰、并基于事实推理的回答。

| 对比维度       | 传统 RAG                     | GraphRAG                             |
| -------------- | ---------------------------- | ------------------------------------ |
| 检索方式       | 基于向量语义相似度           | 向量+知识图遍历/查询                 |
| 关系理解能力   | 弱：只能匹配语义相近片段     | 强：能理解实体之间的多跳关系与结构   |
| 多跳推理支持   | 弱：难以综合跨文档信息       | 强：图结构天然支持推理路径遍历       |
| 语义上下文覆盖 | 依赖检索片段                 | 可检索完整实体子图、社区摘要         |
| 可解释性       | 中：返回片段但缺关键信息结构 | 高：能显示实体关系路径及社区结构     |
| 性能/复杂度    | 低：直接使用向量库           | 高：需要图构建、遍历、摘要等pipeline |

传统 RAG 主要是“先检索语义近似片段，再生成回答”，适合简单查询与短对话。但当问题需要“连接多个事实”“推理关系链”和“洞察上下文结构”时，传统 RAG 会显得力不从心，而 GraphRAG 正是为复杂推理场景设计的增强机制。

- 适配当代Agent系统必备技术：Agentic RAG

&emsp;&emsp; **Agentic RAG（Agentic Retrieval-Augmented Generation）** 是一种在传统 RAG 基础上进一步扩展的增强范式，它将**检索增强生成**与**Agent（智能体）能力**有机结合，使大模型不仅能够基于外部知识库进行回答，还能够通过一系列自主决策和工具调用来完成复杂任务。与经典 RAG 的“检索+拼接+生成”线性流程不同，Agentic RAG 将 LLM 视为一个具备推理、规划和操作能力的智能体，它在对话过程中可以根据问题拆解子任务，先后执行多轮检索、知识整合、函数调用甚至外部API请求，再将结果动态组合成最终的答案。

&emsp;&emsp; 在这个模式下，大模型可以主动提出接下来的检索需求，或根据中间推理结果迭代获取更多信息，形成“循环式检索与生成”的闭环工作流。例如，当用户提出复杂查询时，Agentic RAG可以先调用检索工具定位候选内容，再使用工具对结果进行归纳或分类，必要时还会触发计算或外部查询操作，最后再汇总所有信息输出一个有依据的、分步骤的解答。

&emsp;&emsp; 相比传统RAG，Agentic RAG不仅提升了回答准确性和透明度，也为多轮推理和跨知识库整合提供了更强的灵活性，是近年来大模型产品中非常重要的能力演进方向。

- RAG系统核心难题：多模态PDF检索

​	而在众多企业级应用场景中，PDF 是最常见且最复杂的文档形式，往往同时包含文字、表格、图片、公式等多模态信息。传统的文本检索方法难以准确解析与索引这些异构内容，导致关键信息无法被有效利用。这使得多模态 PDF 检索成为 RAG 技术落地的核心难题：既要保证结构化信息的还原，又需在语义层面实现跨模态统一检索，从而支撑高质量问答与知识增强应用。

### 6.更多LangChain&RAG技术入门

- 《从零RAG入门与大模型知识库问答系统开发实战！》

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828165636757.png" alt="image-20250828165636757" style="zoom: 33%;" />

- 视频地址：https://www.bilibili.com/video/BV1m682ziE9b/

- 课件资料：

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828165903966.png" alt="image-20250828165903966" style="zoom: 33%;" />

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/f7c49313c41eaeb3a2b3b9e9240d9f1e.png" alt="f7c49313c41eaeb3a2b3b9e9240d9f1e" style="zoom: 25%;" />

## 二、多模态文档检索技术介绍

- 多模态RAG系统开发背景

​	在当今信息环境中，单一模态的检索已无法满足人们的需求。随着图像、视频、音频以及文本等多模态数据的高速增长与普及，知识的呈现方式不再局限于纯文本，更多地以丰富的多模态形式存在。从医学影像到工业监控，从视频课程到社交媒体，核心信息往往蕴含在多模态内容的交叉中。传统的文本检索无法充分利用这些异质信息，导致知识获取存在片面与缺失。多模态RAG（Retrieval-Augmented Generation）系统正是在这一背景下显得愈发重要。它能够跨模态整合信息，提升知识覆盖率与语义理解能力，为用户提供更准确、更全面的回答与洞察。这不仅是技术演进的趋势，更是应对现实复杂信息环境的必然选择。

- 多模态文档样例

  - 风景图

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828170635591.png" alt="image-20250828170635591" style="zoom:50%;" />

  - 表格

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828171953065.png" alt="image-20250828171953065" style="zoom:50%;" />

  - 流程图

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202310292019679.png" alt="Code AI Agent架构图" style="zoom:90%;" />

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828170901639.png" alt="image-20250828170901639" style="zoom:50%;" />

  - 产品图

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828125805000.png" alt="image-20250828125805000" style="zoom:50%;" />

  - latex公式

    &emsp;&emsp;并且，在Lesson 12中我们曾证明，伪残差之所以能够加快模型收敛速度，是因为伪残差代表的拟合方向就是损失函数最快速减小（下降）的方向。换而言之，通过一颗颗决策树不断拟合伪残差，最终能够使得损失函数最快速的减小。同时，在伪残差的具体选取上，GBDT的伪残差是样本的负梯度：
    $$
    r_{it-GBDT} = -\frac{\partial{l(y_i,H_{t-1}(x_i))}}{\partial{H_{t-1}(x_i)}}
    $$
    而XGB的伪残差则是一个同时包含梯度和损失函数二阶导的计算结果：
    $$
    g_{ik-XGB} = \frac{\partial{l(y_i,H_{k-1}(x_i))}}{\partial{H_{k-1}(x_i)}}
    $$

    $$
    h_{ik-XGB} = \frac{\partial^2{l(y_i,H_{k-1}(x_i))}}{\partial{H^2_{k-1}(x_i)}}
    $$

    $$
    r_{ik-XGB} = -\frac{g_{ik}}{h_{ik}}
    $$

    而根据Lesson 13中的数学推导不难看出，从本质上来说，XGB的伪残差是在拟合损失函数的二阶泰勒展开，而GBDT的伪残差则是在拟合损失函数的一阶泰勒展开。在大多数情况下，通过拟合二阶泰勒展开，能够更好的捕捉损失函数的更加细微的变动，从而提升精度，但代价是这么做需要耗费更大的计算量。

  - 手写公式拍照

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828170329345.png" alt="image-20250828170329345" style="zoom:50%;" />

  - 论文图表

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828171030944.png" alt="image-20250828171030944" style="zoom:50%;" />

  - 结构化数据图

    <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828171141446.png" alt="image-20250828171141446" style="zoom:50%;" />

- 多模态系统开发的四个核心环节

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828173356266.png" alt="image-20250828173356266" style="zoom:33%;" />

​	在构建多模态RAG系统的过程中，通常需要经过四个核心步骤。首先是**文档解析**，即将原始的多模态资料（如PDF、视频、音频、图像等）进行结构化处理，确保不同格式的数据能够被系统统一理解和管理。其次是**多模态内容信息提取**，在这一环节中，借助OCR、语音识别、图像标注等技术，将文本、语音、视觉要素转化为可计算的中间表示，从而捕捉潜在的知识点与语义线索。第三步是**多模态信息向量化与存储**，通过预训练的跨模态嵌入模型，将不同模态的内容映射到同一语义空间，并结合向量数据库进行高效存储和索引。最后是**多模态信息检索方法**，在实际应用中通过语义检索、跨模态对齐与重排序等策略，快速定位与用户问题最相关的内容，并将结果交由生成模型进行增强式回答。这一流程既保证了信息处理的系统性，又奠定了多模态RAG在复杂任务中提供精准洞察的基础。

> 这里需要注意，在绝大多数工程环节中，文档结构解析与多模态内容解析是同步进行的。

- 多模态文档检索的核心思路：将文档转化为Markdown格式文档再进行检索

  ​	在多模态RAG系统的实践中，将PDF等复杂文档转化为Markdown格式后再进行检索，已成为一种通用且高效的做法。原因在于，PDF本身是一种排版与展示导向的格式，内部结构常常包含大量冗余信息、复杂布局和非线性内容（如表格、脚注、分页元素等），这使得直接检索难以保证准确性和一致性。而Markdown则提供了一种轻量级的结构化表达方式，能够在保持文档层级、段落与语义逻辑的同时，大幅简化格式复杂度。通过这一转换，文本信息被规整化，图像、表格等多模态元素也能以引用或标记的方式统一嵌入，进而更便于向量化处理与跨模态检索。换言之，Markdown既保留了信息的结构完整性，又为后续多模态信息抽取与语义检索奠定了清晰的基础，从而提升了整个RAG系统的稳定性与可扩展性。

### 1. 多模态文档结构解析技术介绍

​	而在围绕多模态文档进行检索的过程中，首先需要围绕多模态文档中的多模态元素进行精准的识别。在实际的多模态文档检索过程中，单纯依靠文本层面的分词与索引往往难以满足需求，因为PDF、扫描件等文档通常包含复杂的 **版面结构**。因此，必须引入 **文档结构化解析** 作为关键步骤。例如实现如下流程：

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250729183006814.png" alt="image-20250729183006814" style="zoom:50%;" />

整体而言，文档解析可分为以下几个层面：

1. **版面区域划分（Layout Analysis）**
   首先需要对页面进行几何级别的分区，将文档中的 **标题、正文、表格、图像、脚注、页眉页脚** 等区域进行标注和切分。这一过程通常依赖 OCR 模型的检测能力，或使用专门的版面分析模型（如 **LayoutLM、DocTr**）来理解文档的空间排布。
2. **层次结构建模（Hierarchical Structuring）**
   在完成区域划分后，需要识别文档的 **逻辑层级**，例如“章节 → 小节 → 段落 → 句子”。这一结构不仅有助于保持语义上下文的完整性，还能让后续的检索模块能够更好地进行分层召回。例如，当用户检索某一章节主题时，系统能直接定位到对应段落，而非无序的全文搜索。
3. **表格与图表解析（Table & Figure Understanding）**
   对于包含数据的文档而言，表格和图表的解析是难点之一。表格需要经过 **行列结构抽取**，再转化为可索引的结构化数据；图表则需要通过 **图像识别 + 语义标注** 的方式提取数据点和趋势描述。这些信息可以作为检索时的重要补充。
4. **跨模态信息融合（Multimodal Fusion）**
   在多模态场景下，单独处理文本或图像并不足够。解析过程中，需要将 **文本信息、图像内容、表格数据** 进行统一建模。例如，某一科学论文的实验结果可能同时存在于“正文描述 + 数据表格 + 折线图”中，完整检索必须能够跨模态聚合这些信息。

通过上述步骤，文档从最初的“视觉排版格式”被转化为具有 **层次化、结构化和语义化** 的知识表示，从而为后续的向量检索（RAG）、问答生成和多模态分析提供高质量的输入。

> 需要注意的是，一般当我们完成多模态PDF文档解析，就能够顺利的将PDF转化为Markdown了，此时图片会单独保存在本地某个文件夹里，并且以链接的方式插入在当前Markdown中。但如果无法对图片内容信息进行提取，则无法获得完整的文档信息。

### 2. 多模态内容信息提取：图片内容信息提取方法介绍

​	在已完成文档结构化解析之后，图片内容的信息提取是多模态处理的关键组成部分。目前来说，最主流的两类提取图片内容信息的方案是使用OCR（光学字符识别）系统/模型，或者使用多模态大模型（VLM）对图片内容进行理解。这两个技术方案各自有各自不同的侧重点：

- 一般来说，OCR系统往往非常轻量高效，甚至无需GPU（或少量GPU算例）、仅需CPU即可完成图片内容的识别，而且OCR系统发展至今，已能拥有非常强悍的识别能力，无论是手写体还是表格文档，都能精准识别：

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828174206625.png" alt="image-20250828174206625" style="zoom:50%;" />

  但问题是，OCR只能识别字符，对于一些高度结构化且主要以文本内容为主的图片（例如表格、发票、公式等）拥有较好的识别效果，但OCR系统无法对图片的含义进行理解，例如产品示意图、流程图等，都无法顺利识别。

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828175130608.png" alt="image-20250828175130608" style="zoom:50%;" />

  > 此外，得益于光学字符识别的技术特性，OCR模型还能够对文档中的各元素结构进行识别，因此在文档解析阶段，最常用的模型就是OCR模型。

- 而相比之下，多模态大模型（VLM）则拥有更加通用的识别能力，无论是图片识别，

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828180421843.png" alt="image-20250828180421843" style="zoom:50%;" />

  ![image-20250828180649068](https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828180649068.png)

  还是图片识别+推理：

  <img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/5013e507-3cfd-4cda-af18-4f3b4e70f8a7.png" alt="5013e507-3cfd-4cda-af18-4f3b4e70f8a7" style="zoom:50%;" />

  都不在话下。但唯一的问题就是只有最顶尖的大模型才具备如此高精准度的识别+推理能力，调用费用不菲，而且还会面临数据隐私安全性风险，虽然有部分开源VLM模型，但往往参数规模较大，本地部署需要较高的硬件成本。

### 3. 多模态PDF文档切分&检索常用流程

​	在 RAG 技术落地过程中，**如何高效解析 PDF 并统一成可检索的多模态数据格式**，是一个绕不开的核心问题。常见的通用套路主要包括以下几类：

#### 3.1 **基于结构解析的完整重建法**

这一流程的核心思想是：先整体识别 PDF 的版面结构，再逐一解析其中的元素。具体步骤如下：

- **版面解析**：识别文档的整体结构，包括页面、段落、标题、表格、图片与公式等元素。
- **元素分离**：
  - 文本部分保留原有的段落与层级结构，避免丢失上下文语义；
  - 图片、表格、公式等多模态内容单独提取，并在本地保存为独立文件；
- **筛选识别**：
  - 对于承载关键信息的多模态内容（如表格、公式、示意图），调用 OCR 或其他专用模型进行文字化处理；
  - 对于装饰性图像或无关插图，则直接忽略，减少冗余。
- **统一转换**：将上述内容转化为 Markdown 文档，文本与多模态对象通过占位符或链接进行关联。
- **检索阶段**：最终将生成的 Markdown 作为知识库输入 RAG 系统，既能保证文本段落的连续性，又能保留多模态内容的溯源能力。

这种方法的优点在于：**结构还原最完整**，适合对学术论文、技术白皮书等结构化程度高的 PDF 进行精细化检索。

#### 3.2 **轻量化切分与多模态并行存储法**

这种方法更强调效率，流程如下：

- 将 PDF 切分为多个逻辑单元（如页、段落、图片对象），不追求完整的版面还原；
- 文本单元直接进行向量化嵌入；
- 图片、表格等非文本元素，保留原始文件路径或转存为图像向量（通过 CLIP、BLIP2 等视觉模型处理）；
- 检索时，文本查询可直接匹配文本嵌入，多模态查询（如带图的检索）则在视觉向量库中进行比对；
- 最终结果由融合器进行多模态合并与排序。

该方法的优点是：**速度快、扩展性强**，特别适合对非结构化 PDF（如报告、说明书、合同）进行快速索引和多模态查询。

#### 3.3 **知识单元抽取与语义增强法**

该流程偏向语义驱动，重点在于信息抽取而非格式还原：

- 在解析 PDF 后，不仅保留原始元素，还通过信息抽取模型识别出 **关键实体、关系和事件**；
- 对表格进行结构化存储，例如转为 CSV 或数据库格式，便于后续 SQL/RAG 混合查询；
- 对公式、代码等特殊元素，转为 LaTeX 或结构化 AST 表示，以便在后续查询中更精确地匹配；
- 将提取出的知识单元与上下文文本一起送入向量库，形成“结构化 + 语义化”的复合检索能力。

优点在于：不仅支持常规的语义检索，还能支持结构化查询（如“查找文档中所有涉及 A 与 B 的关系”），更适合科研、金融、法律等场景。

​	几种方案效果对比如下：

| 流程名称                     | 核心思路                                                     | 优点                                                   | 缺点                           | 适用场景                         |
| ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------ | ------------------------------ | -------------------------------- |
| **结构解析重建法**           | 先识别 PDF 的整体结构，再逐一解析文本、图片、表格、公式等元素，最终统一转为 Markdown | 结构保留最完整，文本与多模态内容可溯源，适合精细化检索 | 处理复杂、耗时较长             | 学术论文、技术白皮书、科研报告   |
| **轻量化并行存储法**         | 将 PDF 切分为文本单元和多模态单元，分别存入文本向量库和图像向量库，检索时并行查询 | 效率高、扩展性强，支持文本和图片混合检索               | 版面和层次结构缺失，语境关联弱 | 报告、说明书、合同等非结构化文档 |
| **知识单元抽取与语义增强法** | 在解析后进一步抽取实体、关系、表格、公式等知识单元，结合文本向量库构建复合检索 | 支持语义检索 + 结构化查询，适合知识发现与推理          | 前处理复杂，需额外的抽取模型   | 金融分析、法律文档、科研数据     |

## 三、多模态RAG系统相关模型&开源项目介绍

### 1. 热门OCR模型介绍

​	在多模态 RAG 技术体系中，**OCR（Optical Character Recognition，光学字符识别）模型依旧是最基础也最重要的一环**。它们的核心价值在于，能够从 PDF、扫描件、票据、图像等文档中，快速而高效地识别出文本信息和基本的版面结构，为后续的向量化与语义检索提供输入。相比大型视觉语言模型（VLM），OCR 模型参数规模更小，推理成本低，甚至可以直接在 CPU 上运行，因而在大规模批处理和轻量化部署场景中有着无可替代的优势。不过，它们的局限也非常明显——通常只能做到文字级别的检测与识别，缺乏对图像内容的语义理解和推理能力。因此，OCR 常常作为「底层解析引擎」存在，与上层的 VLM 模型或产品工具相配合，形成完整的多模态信息处理流水线。

#### 1.1  **dots.ocr**

​	dots.ocr是小红书近期发布的OCR大模型。不同于传统 OCR 工具链依赖「检测 → 识别 → 版面重构」的多阶段流水线，dots.ocr 采用了统一的 Vision-Language Transformer 架构，将版面检测、文字识别和结构解析融为一体。这种设计极大减少了模块之间的对齐误差，使得模型在多语种文档、复杂版面和表格场景中表现出色。凭借仅 1.7B 的参数规模，dots.ocr 兼顾了轻量与高精度，被视为在“端到端文档解析”方向的重要突破。它的出现不仅推动了 OCR 技术向一体化演进，也为构建更高效的多模态 RAG 系统提供了新的底层支撑。

- **优势**：单模架构减少流水线对齐误差；在多语言与复杂版面上表现突出；易用的 prompt 化任务切换（布局/表格/文本）。
- **局限**：社区反馈在少数复杂表格（合并单元格）场景仍需微调或后处理。
- **适用**：论文/报告、票据类文档的端到端解析；希望降低多模型编排成本的团队。

- 项目地址：https://github.com/rednote-hilab/dots.ocr

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828184923520.png" alt="image-20250828184923520" style="zoom:50%;" />

- 运行效果

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828185104830.png" alt="image-20250828185104830" style="zoom:50%;" />

#### 1.2 **olmOCR**（Allen AI）

​	在轻量 OCR 工具中，**olmOCR 的特色在于对复杂 PDF 与扫描文档进行“线性化还原”**。它由 **Allen Institute for AI (AI2)** 团队于 2024 年开源，核心目标是最大限度地保持文档阅读顺序的完整性，同时兼顾表格、公式以及手写体等特殊内容的识别。olmOCR 的模型规模属于**中小尺寸**，总共7B参数两，可以在常规 GPU 环境甚至部分 CPU 配置下运行，适合科研与生产场景的快速部署。与传统 OCR 偏重“字符识别”不同，olmOCR 更强调文档的整体可读性与内容一致性，因此在大规模 PDF 转文本的批处理场景下表现突出，是学术界和产业界逐渐关注的高保真 OCR 工具。

- **优势**：对复杂排版的读序恢复能力强；手写体/公式覆盖；开箱即用。
- **局限**：定位于“文本线性化”，对图像语义本身不做高级理解（需上层 VLM）。
- **适用**：海量 PDF 到可检索文本的高质量批处理；RAG 预处理。

- 项目地址：https://github.com/allenai/olmocr

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828185356142.png" alt="image-20250828185356142" style="zoom:50%;" />

- 运行效果

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828185556488.png" alt="image-20250828185556488" style="zoom:50%;" />

#### 1.3 **PaddleOCR**

​	作为最成熟的开源 OCR 工具链之一，**PaddleOCR** 由 **百度飞桨（PaddlePaddle）团队**自 2020 年起持续维护与迭代，至今已覆盖数十种模型和场景。其模型规模从**轻量级 3–10M 参数的 CPU 可运行版本**，到**上百 MB 的高精度模型**均有覆盖，用户可以根据硬件条件与精度需求灵活选择。PaddleOCR 的优势在于模型生态完整，涵盖文本检测、识别到版面分析的全流程，且原生支持多语种（含中文、英文、日文、韩文等 80+ 语言）。凭借优化的推理性能和丰富的部署方案（服务器、移动端、嵌入式），它已经在票据识别、发票解析、工业表单处理等领域被广泛应用。虽然在复杂表格、跨页排版等语义层面仍需额外规则或上层模型辅助，但凭借其**大规模用户群体与长期工程化打磨**，PaddleOCR 已成为工业界 OCR 的事实标准。

- **优势**：生态完善、文档与示例丰富、轻量模型可 CPU 运行；在特定流水线中官方示例强调“毫秒级”预测与灵活服务部署。
- **局限**：对复杂表格/图表/跨页关系仍需规则/二次建模；语义理解需与上层模块结合。
- **适用**：端侧/低成本批处理、工程化稳定大规模 OCR 服务。

- 项目地址：https://github.com/PaddlePaddle/PaddleOCR

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828185623752.png" alt="image-20250828185623752" style="zoom:50%;" />

- 运行效果

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828185736372.png" alt="image-20250828185736372" style="zoom:50%;" />

各类主流OCR模型对比如下：

| 模型          | 发布团队                     | 参数规模                       | 核心特点                                             | 优势                                          | 局限                                      | 适用场景                              |
| ------------- | ---------------------------- | ------------------------------ | ---------------------------------------------------- | --------------------------------------------- | ----------------------------------------- | ------------------------------------- |
| **dots.ocr**  | HiLab 社区 / 开源社区        | ~1.7B                          | 统一 VLM 架构，版面检测+字符识别一体化，多语种       | 精度高，结构保持好，复杂表格/论文解析表现突出 | 合并单元格等极端结构仍需微调；需 GPU 运行 | 论文、技术文档、票据类端到端解析      |
| **olmOCR**    | Allen Institute for AI (AI2) | 7B                             | 高保真文档线性化，保持阅读顺序，支持表格/公式/手写体 | 可在普通 GPU/部分 CPU 上运行，文本还原度高    | 偏重文本线性化，缺乏图像语义理解          | 大规模 PDF 转文本，科研/学术批处理    |
| **PaddleOCR** | 百度飞桨（PaddlePaddle）     | 轻量模型 3–10M；高精度百 MB 级 | 工业级 OCR 工具链，覆盖检测/识别/版面分析，多语种    | CPU/移动端可运行，文档与社区完善，部署稳定    | 表格/跨页结构复杂时需规则或上层模型配合   | 工业票据识别、大规模生产环境 OCR 服务 |

### 2. 热门VLM模型介绍

#### 2.1 在线VLM模型

​	在多模态 RAG 技术体系中，**在线 VLM 模型是目前能力最全面的语义理解引擎**。这类模型往往由顶尖大厂训练并托管在云端，参数规模达到数百亿甚至上千亿，具备强大的多模态感知与推理能力。典型代表包括 **OpenAI 的 GPT-5**（原生支持文本、图像、音频等模态，提供完善的 API 与生态）、**Google 的 Gemini 2.5**（强调长上下文、多语言和与搜索/Workspace 的无缝集成）、以及 **Anthropic 的 Claude 4.1**（在多步推理与代理式任务中表现突出，并已在多云环境提供企业级接入）。这类在线模型的优势在于即开即用、功能齐全、语义理解能力极强，但与此同时也存在调用成本高、隐私合规受限的现实问题。因此，在线 VLM 更适合作为复杂问题的“上层大脑”，在需要深度语义理解、跨模态推理和企业级可靠性的场景下发挥核心价值。

#### 2.2 **InternVL 3.5**模型

​	**InternVL 3.5** 由 **上海人工智能实验室 (Shanghai AI Lab)** 联合多家科研团队于 2025 年发布，是继 InternVL 2.x 系列后的重大更新版本。该模型参数规模覆盖 **8B 至 40B**，在图像理解、表格解析、跨模态检索和复杂推理方面均有显著提升。特别是提出了 **Cascade RL（级联强化学习）** 策略，用于增强模型的多步推理稳定性，使其在图表问答、科学文献解析等任务中表现优于同类开源模型。

- **优势**：推理链条长、跨模态任务表现强，支持多语言和科研级任务；社区生态活跃。
- **局限**：大尺寸模型的显存占用较高，对硬件配置有一定门槛。

- 项目地址：https://github.com/OpenGVLab/InternVL

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828190333723.png" alt="image-20250828190333723" style="zoom:50%;" />

- 运行效果

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828190453737.png" alt="image-20250828190453737" style="zoom: 50%;" />

#### 2.3 Qwen3-VL

**Qwen3-VL** 是 **阿里巴巴达摩院** 在 2025 年推出的最新一代视觉语言模型，是 Qwen2.5-VL 的升级版。其参数规模从 **3B、7B 到 72B**，覆盖轻量部署与高性能需求，具备**目标检测、图表理解、视频解析**等全面能力。Qwen3-VL 在 **跨语言文档解析、长视频理解** 上有增强优化，并延续了 Qwen 系列在企业级开源社区中的强大影响力。

- **优势**：参数规模覆盖广，性能与成本可灵活平衡；对文档/图表解析能力突出。
- **局限**：大尺寸模型需要高端 GPU，推理延迟较大。
- **适用场景**：企业文档检索、长视频内容解析、多语言跨模态问答。
- Qwen2.5模型开源地址：https://github.com/QwenLM/Qwen2.5-VL

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828190936955.png" alt="image-20250828190936955" style="zoom: 50%;" />

#### 2.4 SmolVLM

​	**SmolVLM** 由 **Hugging Face 社区**在 2024 年末发起，是一类 **轻量级 Vision-Language Model**，参数规模通常在 **1B–2B** 左右，主打 **低算力环境可运行**。与大型 VLM 相比，SmolVLM 的目标不是追求极致性能，而是通过紧凑模型结构，在笔记本或中低端 GPU 上也能实现图文问答、图像 caption 等多模态任务。

- **优势**：模型小巧，部署门槛低；训练与调用成本显著低于大型 VLM。
- **局限**：在复杂表格解析、多步推理上的表现明显落后于大模型；在专业场景（科研、法律文档）效果有限。
- **适用场景**：教学实验、个人项目、边缘设备上的轻量多模态应用

- 项目地址：https://github.com/huggingface/smollm

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828191030909.png" alt="image-20250828191030909" style="zoom:50%;" />

- 运行效果

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828191212036.png" alt="image-20250828191212036" style="zoom:50%;" />

#### 2.5 Gemma 3

​	**Gemma 3** 是 **Google DeepMind** 在 2025 年开源的最新多模态模型，提供 **4B、12B、27B** 三个参数规模，支持文本与图像输入。Gemma 3 延续了 Gemma 系列开源、透明、注重轻量化的设计理念，并针对 **图像问答、图表解析** 等任务做了优化。它兼顾了学术研究的可复现性与企业应用的可落地性，尤其在中小规模下提供了性能与算力需求的良好平衡。

- **优势**：覆盖轻量到中型参数规模，支持多模态输入；Google 官方维护，生态完善。
- **局限**：相比更大规模的 VLM（如 GPT-5、InternVL 40B），在复杂推理和长文档解析上能力有限。
- **适用场景**：科研探索、企业试点项目、对成本敏感的多模态应用。
- 模型地址：https://huggingface.co/google/gemma-3-4b-it

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828191331699.png" alt="image-20250828191331699" style="zoom:33%;" />

各类VLM模型对比如下

| 模型             | 发布团队           | 参数规模   | 类型     | 核心特点                                    | 优势                                    | 局限                               | 适用场景                           |
| ---------------- | ------------------ | ---------- | -------- | ------------------------------------------- | --------------------------------------- | ---------------------------------- | ---------------------------------- |
| **GPT-5**        | OpenAI             | 百亿+      | 在线 API | 原生多模态（文本/图像/音频），API 生态完善  | 功能最全，推理强，生态成熟              | 成本高，需考虑隐私合规             | 高阶语义推理，企业级 RAG，代理任务 |
| **Gemini 2.5**   | Google DeepMind    | 数百亿     | 在线 API | 长上下文（百万级），文本/图像/音频/视频融合 | 与搜索/Workspace 深度整合，多模态能力强 | 部署受地区/合规限制                | 长文档检索，复杂企业场景           |
| **Claude 4.1**   | Anthropic          | 百亿+      | 在线 API | 多步推理与代理式任务突出                    | 长程任务表现好，企业接入灵活            | 成本与速率受限，图像能力因版本而异 | 工程/科研多步任务，企业合规环境    |
| **InternVL 3.5** | 上海人工智能实验室 | 8B–40B     | 开源     | Cascade RL 增强推理，图表/跨模态理解        | 推理强，科研友好，社区活跃              | 大模型需高端 GPU                   | 科研论文解析，图表问答             |
| **Qwen3-VL**     | 阿里巴巴达摩院     | 3B/7B/72B  | 开源     | 文档解析、目标定位、长视频理解              | 尺寸覆盖广，性能灵活                    | 大尺寸算力要求高                   | 企业文档、多语言跨模态应用         |
| **SmolVLM**      | Hugging Face 社区  | 1B–2B      | 开源     | 轻量 VLM，低算力可运行                      | 部署门槛低，适合个人/教育               | 复杂任务效果弱                     | 教学、轻量个人项目                 |
| **Gemma 3**      | Google DeepMind    | 4B/12B/27B | 开源     | 轻量到中型参数，图像问答/图表解析           | 成本低，生态完整                        | 性能弱于超大模型                   | 成本敏感型企业/科研试点            |

### 3. 多模态PDF转Markdown产品级解决方案

#### 3.1 MinerU：高精度 PDF 转 Markdown 的一体化工具

​	**MinerU** 由 **阿里巴巴达摩院与 OpenDataLab 社区**联合开源，是当前性能最突出的 PDF → Markdown 转换工具之一。它集成了 **OCR 模型、版面解析与结构化抽取**，能够处理学术论文、扫描件和复杂排版文档。MinerU 特别在 **公式、表格、图片引用** 等细节保留上表现优异，使得输出的 Markdown 更加接近原始文档语义。

- **优势**：输出结构清晰、对数学公式/表格解析精度高；社区活跃、CLI 使用方便。
- **局限**：使用 **AGPL-3.0** 许可证，对闭源商用有限制；在极端复杂排版场景仍可能需要人工后处理。
- **适用场景**：科研 PDF 批量解析、技术文档转换、构建高质量 RAG 知识库的前置步骤。

- 项目地址：https://github.com/opendatalab/MinerU

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828191550594.png" alt="image-20250828191550594" style="zoom:50%;" />

- 使用效果

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250828191755402.png" alt="image-20250828191755402" style="zoom:50%;" />

#### 3.2 Docling：企业级文档解析与知识抽取框架

​	**Docling** 由 **IBM Research** 在 2024 年开源，定位为一个 **企业级文档智能解析平台**。与 MinerU 相比，Docling 的目标并非单一的 PDF → Markdown，而是构建一个覆盖 **PDF、Word、PPT、HTML** 等多格式的完整解析管线。它内置 **OCR 与布局分析组件**，同时支持开发者通过插件方式接入 **VLM 模型（如 LLaVA、Qwen-VL、InternVL 等）**，从而实现更强的多模态理解能力。

- **优势**：支持多种文件格式，输出可为 Markdown/HTML/JSON；**MIT 许可**更宽松，适合企业商用；工程化完备，集成 LangChain/LlamaIndex 等生态。
- **局限**：默认配置下对某些复杂图表或领域专用排版可能需要自定义 pipeline；对硬件环境要求略高于轻量工具。
- **适用场景**：企业级大规模文档解析、知识抽取与合规环境下的离线知识库构建。

因此，Docling 非常适合作为 **研发人员在早期原型开发阶段的快速文档解析工具**，既能保证输出质量，又能降低开发成本。目前它在社区中逐渐流行，被认为是 **介于“轻量工具”和“工业级解析引擎”之间的理想选择**。

- 项目地址：https://github.com/docling-project/docling

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250729184534696.png" alt="image-20250729184534696" style="zoom:50%;" />

#### 3.3 MarkItDown：轻量化的通用文档转 Markdown 工具

​	**MarkItDown** 由 **微软（Microsoft）** 在 2024 年开源，定位为 **轻量通用的文件 → Markdown 转换工具**。它支持 PDF、Office 文档、图像、网页等多种格式，并提供 **插件机制**，可调用 Azure Document Intelligence 或 LLM（如 GPT-4o）对图像、表格等内容进行增强式解析。与 MinerU 和 Docling 相比，MarkItDown 更强调「简单、快速、易集成」，适合开发者在 RAG 项目中快速预处理多格式文档。

- **优势**：**MIT 许可**，开源宽松；支持格式最广，插件体系灵活，可调用 LLM 提升精度；部署轻量，安装简单。
- **局限**：对复杂科研论文或结构化表格的还原精度不如 MinerU；默认输出更注重可读性而非高精度复现。
- **适用场景**：需要快速集成多格式文档解析的 RAG 应用；轻量项目或对精度要求不高的知识库构建。

- 目地址：https://github.com/microsoft/markitdown/

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250729183952918.png" alt="image-20250729183952918" style="zoom:33%;" />

各类产品级解决方案对比如下：

| 工具           | 发布团队                     | 许可证   | 支持格式                    | 核心特点                                         | 优势                               | 局限                               | 适用场景                         |
| -------------- | ---------------------------- | -------- | --------------------------- | ------------------------------------------------ | ---------------------------------- | ---------------------------------- | -------------------------------- |
| **MinerU**     | 阿里巴巴达摩院 & OpenDataLab | AGPL-3.0 | PDF/图像                    | 集成 OCR+版面解析，高精度公式/表格支持           | 输出精细，科研 PDF 友好，CLI 简洁  | 商用许可受限，复杂排版仍需人工校正 | 科研论文/技术文档批量转 Markdown |
| **Docling**    | IBM Research                 | MIT      | PDF/Word/PPT/HTML 等        | 企业级文档解析，支持接入 VLM，输出 Markdown/JSON | 多格式支持，工程化完备，可离线运行 | 默认复杂图表需自定义管线           | 企业合规环境，知识库构建         |
| **MarkItDown** | Microsoft                    | MIT      | PDF/Office/图片/网页/音频等 | 轻量通用工具，插件可接入 Azure/LLM               | 格式覆盖广，插件灵活，部署简单     | 精排不及 MinerU，偏重可读性        | 快速预处理，通用 RAG 项目集      |

---

- 体验课内容节选自[《2025大模型Agent智能体开发实战》(秋招冲刺班)](https://ix9mq.xetslk.com/s/2S2Vpy)完整版付费课程

&emsp;&emsp;体验课时间有限，若想深度学习大模型技术，欢迎大家报名由我主讲的[《2025大模型Agent智能体开发实战》(秋招冲刺班)](https://ix9mq.xetslk.com/s/2S2Vpy)

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/06661cb459aa3e4b655aface404435d.png" alt="06661cb459aa3e4b655aface404435d" style="zoom:15%;" />

**[《2025大模型Agent智能体开发实战》(秋招冲刺班)](https://ix9mq.xetslk.com/s/2S2Vpy)为【100+小时】体系大课，总共20大模块精讲精析，零基础直达大模型企业级应用！**

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506172010074.png" alt="a55d48e952ed59f8d93e050594843bc" style="zoom:50%;" />

### 部分课程成果演示

- Dify+DeepSeek搭建智能客服

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4"></video>

- Coze自动图文视频创作流程

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/Coze%E5%8A%A8%E6%80%81%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E5%AE%9E%E4%BE%8B.mp4"></video>

- 可视化数据分析Multi-Agent

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4"></video>

- Ollama 自动化并发请求测试与动态资源监控

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/3.Ollama%20%E8%87%AA%E5%8A%A8%E5%8C%96%E5%B9%B6%E5%8F%91%E8%AF%B7%E6%B1%82%E6%B5%8B%E8%AF%95%E4%B8%8E%E5%8A%A8%E6%80%81%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7.mp4"></video>

- Neo4j并行多线程导入百万级文本方法与实践

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/2.Neo4j%E5%B9%B6%E8%A1%8C%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%AF%BC%E5%85%A5%E7%99%BE%E4%B8%87%E7%BA%A7%E6%96%87%E6%9C%AC%E6%96%B9%E6%B3%95%E4%B8%8E%E5%AE%9E%E6%88%98%E6%BC%94%E7%A4%BA.mp4"></video>

- 高效微调全自动数据集创建

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/easy_daset_yanshi.mp4"></video>

- MateGen Pro 项目功能演示

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/MG%E6%BC%94%E7%A4%BA%E8%A7%86%E9%A2%91.mp4"></video>

- 智能客服项目展示

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E8%A7%86%E9%A2%91.mp4"></video>

- **GraphRAG+多模态文档检索**

<video src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/7%E6%9C%8817%E6%97%A5%281%29%20%E8%BF%9B%E5%BA%A6%E6%9D%A1.mp4"></video>

此外，若是对大模型底层原理感兴趣，也欢迎报名由我和菜菜老师共同主讲的[《2025大模型原理与实战课程》(秋招冲刺班)](https://ix9mq.xetslk.com/s/3AME7R)

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/202506171650709.png" alt="4a11b7807056e9f5b281278c0e37dad" style="zoom:20%;" />

**大模型秋招冲刺班开班特惠进行中，直播间享五折特价+全套SVIP新班特定福利，合购还有更多优惠哦~<span style="color:red;">详细信息扫码添加助教，回复“大模型”，即可领取课程大纲&查看课程详情👇</span>**

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/26449c9c3e90ea66e0af9150ad00e0c6.png" alt="26449c9c3e90ea66e0af9150ad00e0c6" style="zoom:50%;" />

<img src="https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/0d489e9d3c29af2d7f319171eac040d0.png" alt="0d489e9d3c29af2d7f319171eac040d0" style="zoom:50%;" />
